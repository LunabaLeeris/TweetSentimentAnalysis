{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the text to vectors\n",
    "First we will convert each text to feature vectors. Specifically we will use a feature vector $X$ where $X_{ij}$ corresponds to the number of occurence of the $j'th$ word (based on our word mapping) in the $i'th$ text\n",
    "\n",
    "For the word mapping, we will first use the top 10,000 words in the english dictionary then add additional words as we scan the texts up to a certain limit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse initial words\n",
    "First a list of stop words we're parsed. This stop words will be removed from consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_len_per_class = 500\n",
    "max_words_to_consider: int = 500\n",
    "max_word_len:       int = 15\n",
    "initial_words_path: str = \"dataset\\google-10000-english.txt\"\n",
    "stop_words_path   : str = \"dataset\\stop_words_english.txt\"\n",
    "\n",
    "\n",
    "def parse_initial_words(destination: dict, path: str, add_undefined_token: bool=False, filter_by: dict = {}, limit_len: int = -1) -> None:\n",
    "    if add_undefined_token:\n",
    "        destination[\"$$$\"] = 0\n",
    "    \n",
    "    print(\"Parsing words\")\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            word: str = line.strip()\n",
    "            if (limit_len > -1 and len(destination) == limit_len):\n",
    "                continue\n",
    "            \n",
    "            if (len(word) <= 1 or len(word) >= max_word_len):\n",
    "                continue\n",
    "            \n",
    "            if (word in filter_by):\n",
    "                continue\n",
    "\n",
    "            destination[word] = len(destination)\n",
    "\n",
    "    print(\"parsing succesfull\")\n",
    "\n",
    "words_to_consider: dict = {}\n",
    "stop_words       : dict = {}\n",
    "\n",
    "parse_initial_words(stop_words, stop_words_path, False)\n",
    "#parse_initial_words(words_to_consider, initial_words_path, True, stop_words, max_words_to_consider)\n",
    "\n",
    "print(len(words_to_consider))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize all the pairs\n",
    "Initialize the working pairs. Pairs are denoted as [first, second] where first = 1 and second = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_path: str = \"train/\"\n",
    "c: str = [\"anger\", \"fear\", \"joy\", \"sadness\"]\n",
    "total_class = len(c)\n",
    "total_pairs: int = int((total_class * (total_class - 1))/2)\n",
    "pairs: np.int8 = np.zeros(shape=(total_pairs, 2), dtype=np.int8)\n",
    "\n",
    "at: int = 0\n",
    "for i in range(total_class - 1):\n",
    "    for j in range(i + 1, total_class):\n",
    "        pairs[at, 0] = i\n",
    "        pairs[at, 1] = j\n",
    "        at += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse all data to vectors\n",
    "Read content of each training data per classification line by line then convert them to tensor.\n",
    "Incrase the length of words_to_consider as we read up to a certain limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "vectorized: np.int32 = np.zeros(shape=(total_class, data_len_per_class, max_words_to_consider))\n",
    "\n",
    "def remove_trailing_special_characters(word):\n",
    "    return re.sub(r'[^a-zA-Z]+$', '', word)\n",
    "\n",
    "# Checks text word by word. If a word not in word_to_consider then we add it\n",
    "# unless we already exceeded the max words to consider, or the word is not a\n",
    "# bad candidate. We then update the occurence of the word in its corresponding matrix\n",
    "def process_text(line: str, i: int, line_idx: int):\n",
    "    D: int= len(words_to_consider)\n",
    "\n",
    "    for word in line.split():\n",
    "        word: str = remove_trailing_special_characters(word)\n",
    "\n",
    "        if word not in words_to_consider:\n",
    "            if word in words_to_consider:\n",
    "                continue \n",
    "            \n",
    "            if D >= max_words_to_consider:\n",
    "                continue\n",
    "            \n",
    "            if len(word) <= 1 or len(word) >= max_word_len:\n",
    "                continue\n",
    "            \n",
    "            words_to_consider[word] = D\n",
    "            D += 1\n",
    "\n",
    "        vectorized[i, line_idx, words_to_consider[word]] += 1\n",
    "    \n",
    "print(\"Converting training data to vectors...\")\n",
    "for i in range(total_class):\n",
    "    data_path: str = train_path + c[i] + \".txt\"\n",
    "    \n",
    "    with open(data_path, 'r', encoding='utf-8') as file:\n",
    "        line_idx: int = 0\n",
    "        for line in file:\n",
    "            if (line_idx == data_len_per_class):\n",
    "                break\n",
    "            \n",
    "            process_text(line, i, line_idx)\n",
    "            line_idx += 1\n",
    "\n",
    "print(\"Conversion complete...\")\n",
    "print(\"Size of vector\", vectorized.shape)\n",
    "print(\"Total words to consider: \", len(words_to_consider))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train each pair\n",
    "Train each pair using the modified SMO class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run SMO.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"smo = SMO_GAUSSIAN(np.concatenate((vectorized[0], vectorized[1]), axis=0), data_len_per_class * 2, max_words_to_consider, c=.5, log=True)\n",
    "smo.smo_train()\n",
    "\n",
    "print(smo.accuracy())\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "np.random.seed(200)\n",
    "\n",
    "n = 3000\n",
    "M = n*2     # Overwrites the above M to make this wor\n",
    "D = 2       # Ensures dimensions are 2\n",
    "\n",
    "def gen_circle(n, center_x=0, center_y=0, radius=1, label=0):\n",
    "    alpha = 2 * np.pi * np.random.rand(n)\n",
    "    r = radius * np.sqrt(np.random.rand(n))\n",
    "    x = r * np.cos(alpha) + center_x\n",
    "    y = r * np.sin(alpha) + center_y\n",
    "    label = np.ones(n) * label\n",
    "    return [x, y, label]\n",
    "\n",
    "C0 = gen_circle(n, center_x=.7, center_y=0, radius=1, label=1)\n",
    "C1 = gen_circle(n, center_x=-.7, center_y=0, radius=1, label=-1)\n",
    "\n",
    "x0 = np.append(C0[0], C1[0])\n",
    "x1 = np.append(C0[1], C1[1])\n",
    "\n",
    "X = np.c_[x0, x1]\n",
    "Y = np.append(C0[2], C1[2])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_x = scaler.fit_transform(X)\n",
    "\n",
    "# Main function\n",
    "point = train_x\n",
    "target = Y\n",
    "model = SMO_GAUSSIAN(train_x, target, M, 2, c=1, log=True)\n",
    "model.smo_train()\n",
    "\n",
    "# Prediction in vector\n",
    "train_y = model.predict(point, new=False)\n",
    "#print('support vector: {} / {}'\\\n",
    "    #.format(len(model['alphs'][model['alphs'] > 1e-5]), len(model['alphs'])))\n",
    "\n",
    "# Gathers the support vectors (non-zero alpha)\n",
    "sv_threshold = 0\n",
    "sv_idx = []\n",
    "for idx, alpha in enumerate(model.alphs):\n",
    "    if alpha > sv_threshold:\n",
    "        #print('index = {}, alpha = {:.3f}, predict y={:.3f}'\\\n",
    "            #.format(idx, alpha, train_y[idx]))\n",
    "        \n",
    "        sv_idx.append(idx)\n",
    "\n",
    "# Threshold\n",
    "print(f'bias = {model.B}')\n",
    "# Error rate\n",
    "train_y_sign = np.sign(train_y)\n",
    "error_rate = np.mean(train_y_sign != target)\n",
    "print('training data error rate = {:.2f}'.format(error_rate))\n",
    "\n",
    "# Draw the Plot\n",
    "plt.plot(C0[0], C0[1], 'o', markerfacecolor='r', markeredgecolor='None', alpha=0.55)\n",
    "plt.plot(C1[0], C1[1], 'o', markerfacecolor='b', markeredgecolor='None', alpha=0.55)\n",
    "\n",
    "resolution = 50\n",
    "dx = np.linspace(X[:, 0].min(), X[:, 0].max(), resolution)\n",
    "dy = np.linspace(X[:, 1].min(), X[:, 1].max(), resolution)\n",
    "dx, dy = np.meshgrid(dx, dy)\n",
    "plot_x = np.c_[dx.flatten(), dy.flatten()]\n",
    "\n",
    "transformed_plot_x = scaler.transform(plot_x)\n",
    "dz = model.predict(transformed_plot_x, True)\n",
    "dz = dz.reshape(dx.shape)\n",
    "\n",
    "plt.contour(dx, dy, dz, alpha=1, colors=('b', 'k', 'r'), \\\n",
    "            levels=(-1, 0, 1), linestyles = ('--', '-', '--'))\n",
    "\n",
    "label_cnt = 0\n",
    "for i in sv_idx:\n",
    "    if label_cnt == 0:\n",
    "        plt.scatter(X[i, 0], X[i, 1], marker='*', color='k', \\\n",
    "                    s=120, label='Support vector')\n",
    "        label_cnt += 1\n",
    "        continue\n",
    "\n",
    "    plt.scatter(X[i, 0], X[i, 1], marker='*', color='k', s=120)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
