{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the text to vectors\n",
    "First we will convert each text to feature vectors. Specifically we will use a feature vector $X$ where $X_{ij}$ corresponds to the number of occurence of the $j'th$ word (based on our word mapping) in the $i'th$ text\n",
    "\n",
    "For the word mapping, we will first use the top 10,000 words in the english dictionary then add additional words as we scan the texts up to a certain limit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse initial words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_word_len: int = 15\n",
    "initial_words_path: str = \"dataset\\google-10000-english.txt\"\n",
    "\n",
    "def parse_initial_words(words_to_consider, path, add_undefined_token=False):\n",
    "    if add_undefined_token:\n",
    "        words_to_consider[\"$$$\"] = 0\n",
    "    \n",
    "    print(\"parsing initial words to consider\")\n",
    "    with open(path, 'r') as file:\n",
    "        for line in file:\n",
    "            word = line.strip()\n",
    "            if (len(word) > 1 and len(word) >= max_word_len):\n",
    "                continue\n",
    "\n",
    "            words_to_consider[word] = len(words_to_consider)\n",
    "\n",
    "    print(\"parsing succesfull\")\n",
    "\n",
    "words_to_consider = {}\n",
    "parse_initial_words(words_to_consider, initial_words_path, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize all the pairs\n",
    "Initialize the working pairs. Pairs are denoted as [first, second] where first = 1 and second = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_path: str = \"train/\"\n",
    "c: str = [\"anger\", \"fear\", \"joy\", \"sadness\"]\n",
    "total_class = len(c)\n",
    "total_pairs: int = int((total_class * (total_class - 1))/2)\n",
    "pairs: np.int8 = np.zeros(shape=(total_pairs, 2), dtype=np.int8)\n",
    "\n",
    "at: int = 0\n",
    "for i in range(total_class - 1):\n",
    "    for j in range(i + 1, total_class):\n",
    "        pairs[at, 0] = i\n",
    "        pairs[at, 1] = j\n",
    "        at += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse all data to vectors\n",
    "Read content of each training data per classification line by line then convert them to tensor.\n",
    "Incrase the length of words_to_consider as we read up to a certain limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words_to_consider: int = 20_000\n",
    "\n",
    "vectorized: np.int32 = np.zeros(shape=(total_class, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
